torchrun --nnodes=1 --nproc_per_node=4 train.py \
  --local_dir "/tmp/llama/" \
  --input_model_filename "Qwen/Qwen3-4B" \
  --output_model_filename "Qwen/Qwen3-4B-4bit" \
  --train_data_local_path "/tmp/train.jsonl" \
  --num_eval 100 \
  --qat True \
  --w_bits 4 \
  \
  --do_train True \
  --do_eval False \
  --fp16 False \
  --bf16 True \
  --tf32 False \
  --gradient_checkpointing False \
  \
  --evaluation_strategy no \
  --save_strategy steps \
  --save_steps 2000 \
  --save_total_limit 1 \
  --logging_steps 1 \
  --learning_rate 2e-5 \
  --weight_decay 0.0 \
  --warmup_ratio 0.0 \
  --lr_scheduler_type cosine \
  --num_train_epochs 1 \
  --per_device_train_batch_size 2 \
  --per_device_eval_batch_size 1 \
  --gradient_accumulation_steps 1 \
  --report_to tensorboard \
  --logging_dir /tmp/output/runs/current \
  --disable_tqdm True
